{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/45664104/86419986-e932eb00-bccc-11ea-98e3-e1dcba512804.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_PATH = \"../datasets/supervisely\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Cloning into 'supervisely-to-darknet'...\n"
    }
   ],
   "source": [
    "# Clone the repo which contains essential functions\n",
    "!git clone https://github.com/JinhangZhu/supervisely-to-darknet.git\n",
    "import sys\n",
    "sys.path.append('./supervisely-to-darknet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from convert import *\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Person: 100%|██████████| 21/21 [00:00<00:00, 21472.55it/s]\n"
    }
   ],
   "source": [
    "# Delete all separate classes.names within each person dataset\n",
    "for p in tqdm(os.listdir(DS_PATH), desc='Person'):\n",
    "    filepath = os.path.join(DS_PATH, p)\n",
    "    if os.path.isdir(filepath):\n",
    "        if os.path.isfile(filepath + './classes.names'):\n",
    "            os.remove(filepath + './classes.names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nSave labels for person/subset/images in ./ds_class_names.json.\nP01 :['hand', '1', 'left_hand', 'right_hand']\nP02 :['hand']\nP03 :['hand']\nP04 :['hand']\nP05 :['hand']\nP06 :['hand']\nP07 :['hand']\nP08 :['hand']\nP10 :['hand']\nP12 :['hand']\nP13 :['hand']\nP14 :['hand']\nP15 :['hand']\nP16 :['hand']\nP17 :['hand']\nP19 :['hand']\nP20 :['hand']\nP21 :['hand']\nP22 :['hand']\nP23 :['hand']\n"
    }
   ],
   "source": [
    "class_names = {}    # Class names for person\n",
    "\n",
    "for p in os.listdir(DS_PATH):\n",
    "    filepath = os.path.join(DS_PATH, p)\n",
    "    # print('\\nPerson {}: '.format(p), filepath)\n",
    "\n",
    "    if os.path.isdir(filepath):\n",
    "        # Read meta.json to get classes\n",
    "        meta_path = filepath + os.sep + 'meta.json'\n",
    "        if os.path.isfile(meta_path):\n",
    "            classes, names_path = get_classes(meta_path, write=False)\n",
    "            class_names[p] = classes\n",
    "        else:\n",
    "            print('There is no meta.json file for person ', p)\n",
    "\n",
    "# Save class names of all datasets into json\n",
    "# class_names_path = DS_PATH + './ds_class_names.json'\n",
    "class_names_path = './ds_class_names.json'\n",
    "print('\\nSave labels for person/subset/images in {}.'.format(class_names_path))\n",
    "with open(class_names_path, 'w') as fp:\n",
    "    json.dump(class_names, fp, indent=4)\n",
    "\n",
    "# Iterate over key/value pairs in dict and print them\n",
    "for p, clss in class_names.items():\n",
    "    print(p, ' : ', clss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出来，整体的标注信息表明，这20个人的数据集里，只有第一个人的数据包含四种标签，其余所有人的数据都是只有`hand`的标签，也就是有手和无手的区别。\n",
    "下一步看：\n",
    "\n",
    "- 各个标签的数目/该数据集的**有**标注总数/照片总数\n",
    "\n",
    "> 但是`1`这个标签实在是很奇怪，需要继续看。对于左右手的标签，我随便点开了几个标注文件，发现没有左右手的，可能也是个占小部分的标注，也需要拿出来看看，是哪些照片有这些小比例的标注，这个比例有多少？总数又是多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read json of class names\n",
    "# with open(class_names_path, 'r') as fp:\n",
    "#     class_names = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_labels(sub_p_path):\n",
    "    \"\"\"Get the labels from a subset of a person.\n",
    "\n",
    "    Argument:\n",
    "        sub_p_path: the path of the subset. e.g. '../datasets/supervisely\\\\P23\\\\P23_01'\n",
    "    \n",
    "    Returns:\n",
    "        sub_p_labels: a dictionary whose keys are image names, values are a list of tuples.\n",
    "            Each tuple is a bounding box: (class_name, b_x_center, b_y_center, b_width, b_height)\n",
    "    \"\"\"\n",
    "    sub_p_labels={}\n",
    "\n",
    "    # Get all file real paths\n",
    "    read_path = sub_p_path + os.sep\n",
    "    ann_paths = sorted(glob.glob(read_path + 'ann/' + '*.json'))\n",
    "    img_paths = sorted(glob.glob(read_path + 'img/' + '*.jpg'))\n",
    "\n",
    "    # Import all json annotation files for images\n",
    "    for (ann_path, img_path) in zip(ann_paths, img_paths):\n",
    "        # Current image\n",
    "        img_name = os.path.basename(img_path)[:-4]\n",
    "        sub_p_labels[img_name] = []\n",
    "\n",
    "        # Import json\n",
    "        with open(ann_path) as ann_f:\n",
    "            ann_data = json.load(ann_f)\n",
    "        \n",
    "        # Image size\n",
    "        image_size = ann_data['size']   # dict: {'height': , 'width': }\n",
    "\n",
    "        # Objects bounding boxes\n",
    "        bboxes = ann_data['objects']\n",
    "        if len(bboxes) != 0:    # With object(s)\n",
    "            for bbox in bboxes:\n",
    "                class_name = bbox['classTitle']\n",
    "                corner_coords = bbox['points']['exterior']  # bbox corner coordinates in [[left, top], [right, bottom]]\n",
    "\n",
    "                # Normalisation\n",
    "                b_x_center = (corner_coords[0][0] + corner_coords[1][0]) / 2 / image_size['width']\n",
    "                b_y_center = (corner_coords[0][1] + corner_coords[1][1]) / 2 / image_size['height']\n",
    "                b_width = (corner_coords[1][0] - corner_coords[0][0]) / image_size['width']\n",
    "                b_height = (corner_coords[1][1] - corner_coords[0][1]) / image_size['height']\n",
    "\n",
    "                # Save bbox label as a tuple for the image\n",
    "                sub_p_labels[img_name].append(\n",
    "                    (\n",
    "                        class_name,\n",
    "                        round(b_x_center, 6),\n",
    "                        round(b_y_center, 6),\n",
    "                        round(b_width, 6),\n",
    "                        round(b_height, 6)\n",
    "                    )\n",
    "                ) \n",
    "        \n",
    "    return sub_p_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Person:   0%|          | 0/21 [00:00<?, ?it/s]1004 image-annotation pairs in subset P01_02 of person P01\n238 image-annotation pairs in subset P01_03 of person P01\n984 image-annotation pairs in subset P01_06 of person P01\nPerson:  10%|▉         | 2/21 [00:00<00:03,  5.91it/s]2070 image-annotation pairs in subset P02_06 of person P02\n134 image-annotation pairs in subset P02_08 of person P02\n109 image-annotation pairs in subset P02_11 of person P02\nPerson:  14%|█▍        | 3/21 [00:00<00:03,  4.68it/s]219 image-annotation pairs in subset P03_06 of person P03\n215 image-annotation pairs in subset P03_07 of person P03\n106 image-annotation pairs in subset P03_11 of person P03\n49 image-annotation pairs in subset P03_18 of person P03\n195 image-annotation pairs in subset P03_27 of person P03\nPerson:  19%|█▉        | 4/21 [00:00<00:03,  4.81it/s]394 image-annotation pairs in subset P04_21 of person P04\n777 image-annotation pairs in subset P05_04 of person P05\n312 image-annotation pairs in subset P05_05 of person P05\nPerson:  29%|██▊       | 6/21 [00:01<00:02,  5.25it/s]181 image-annotation pairs in subset P06_01 of person P06\n28 image-annotation pairs in subset P06_02 of person P06\n1371 image-annotation pairs in subset P06_03 of person P06\n62 image-annotation pairs in subset P06_08 of person P06\nPerson:  33%|███▎      | 7/21 [00:01<00:02,  4.67it/s]224 image-annotation pairs in subset P07_02 of person P07\n346 image-annotation pairs in subset P07_08 of person P07\n447 image-annotation pairs in subset P07_11 of person P07\nPerson:  38%|███▊      | 8/21 [00:01<00:02,  4.77it/s]215 image-annotation pairs in subset P08_02 of person P08\n300 image-annotation pairs in subset P08_03 of person P08\n48 image-annotation pairs in subset P08_07 of person P08\n68 image-annotation pairs in subset P08_12 of person P08\n275 image-annotation pairs in subset P08_19 of person P08\nPerson:  43%|████▎     | 9/21 [00:01<00:02,  4.76it/s]1824 image-annotation pairs in subset P10_01 of person P10\nPerson:  48%|████▊     | 10/21 [00:02<00:02,  4.53it/s]121 image-annotation pairs in subset P12_05 of person P12\n340 image-annotation pairs in subset P12_06 of person P12\n223 image-annotation pairs in subset P13_06 of person P13\nPerson:  57%|█████▋    | 12/21 [00:02<00:01,  5.83it/s]209 image-annotation pairs in subset P14_01 of person P14\n65 image-annotation pairs in subset P14_02 of person P14\n40 image-annotation pairs in subset P14_03 of person P14\n94 image-annotation pairs in subset P15_01 of person P15\n55 image-annotation pairs in subset P15_11 of person P15\n112 image-annotation pairs in subset P16_02 of person P16\n426 image-annotation pairs in subset P16_03 of person P16\nPerson:  71%|███████▏  | 15/21 [00:02<00:00,  7.14it/s]313 image-annotation pairs in subset P17_01 of person P17\n273 image-annotation pairs in subset P19_03 of person P19\nPerson:  81%|████████  | 17/21 [00:02<00:00,  8.67it/s]426 image-annotation pairs in subset P20_03 of person P20\n1138 image-annotation pairs in subset P21_04 of person P21\nPerson:  90%|█████████ | 19/21 [00:02<00:00,  8.05it/s]853 image-annotation pairs in subset P22_09 of person P22\n184 image-annotation pairs in subset P23_01 of person P23\nPerson: 100%|██████████| 21/21 [00:02<00:00,  7.03it/s]\n\nSave labels for person/subset/images in labels.json.\n"
    }
   ],
   "source": [
    "# Labels of all datasets of all person\n",
    "# format:\n",
    "# {\n",
    "#   'person':{\n",
    "#               'subset':{\n",
    "#                           'image': [(*bbox attributes)]\n",
    "#                        }                \n",
    "#            }   \n",
    "# }\n",
    "labels = {}\n",
    "\n",
    "# Collect all annotations into a single json\n",
    "for p in tqdm(os.listdir(DS_PATH), desc='Person'):\n",
    "    p_path = os.path.join(DS_PATH, p)\n",
    "\n",
    "    if os.path.isdir(p_path): \n",
    "        labels[p] = {}   # 'person'\n",
    "\n",
    "        for sub_p in os.listdir(p_path):\n",
    "            sub_p_path = os.path.join(p_path, sub_p)\n",
    "\n",
    "            if os.path.isdir(sub_p_path):\n",
    "                labels[p][sub_p] = get_subset_labels(sub_p_path)    # 'subset'\n",
    "                print(\n",
    "                    \"{} image-annotation pairs in subset {} of person {}\".format(len(labels[p][sub_p]), sub_p, p)\n",
    "                )\n",
    "\n",
    "labels_path = 'labels.json'\n",
    "print('\\nSave labels for person/subset/images in {}.'.format(labels_path))\n",
    "with open(labels_path, 'w') as fp:\n",
    "    json.dump(labels, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到少数类的标注所在的图片|以及数目|以及总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_annotation(labels, annotation):\n",
    "    r_with_anno = False\n",
    "    for p, p_labels in labels.items():\n",
    "        for sub_p, sub_p_labels in p_labels.items():\n",
    "\n",
    "            imgs_with_anno = []\n",
    "            for img_name, bboxes in sub_p_labels.items():\n",
    "                if len(bboxes) > 0:\n",
    "                    for bbox in bboxes: # bbox: tuples of five attributes\n",
    "                        if bbox[0] == annotation:\n",
    "                            imgs_with_anno.append(img_name)\n",
    "                            break\n",
    "            \n",
    "            if len(imgs_with_anno) > 0 and r_with_anno is False:\n",
    "                r_with_anno = True  # There exists such an image with such annotation\n",
    "\n",
    "            # print(\n",
    "            #     \"For subset {}, images with annotation '{}' count {}/{}.\".format(sub_p, annotation, len(imgs_with_anno), len(sub_p_labels))#,\n",
    "            # #    imgs_with_anno\n",
    "            # )\n",
    "    return r_with_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No image with annotation: 'left_hand'\nNo image with annotation: 'right_hand'\nNo image with annotation: '1'\nImage exists with annotation: 'hand'\n"
    }
   ],
   "source": [
    "# Get all labels from class_names (for all person)\n",
    "all_annotations = []\n",
    "for clss in class_names.values():\n",
    "    all_annotations.extend(clss)\n",
    "all_annotations = set(all_annotations)\n",
    "for annotation in all_annotations:\n",
    "    if not find_annotation(labels, annotation):\n",
    "        print(\"No image with annotation: '{}'\".format(annotation))\n",
    "    else:\n",
    "        print(\"Image exists with annotation: '{}'\".format(annotation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在问题很清晰了，第一个人的数据集根本没有我开始认为的少数标签：`1`, `left_hand`, `right_hand`，猜想是最开始设置了这些但是实际标注只标注了`hand`的标签。\n",
    "\n",
    "所以得到一个结论：**这一次下载的所有的数据集都只有`hand`的标签**。可以把第一个人的`meta.json`的无用标签删掉了。\n",
    "\n",
    "下一步，**数据清洗`hand`数据集，生成anchors，完成训练**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['left_hand', 'right_hand', '1']\n"
    }
   ],
   "source": [
    "del_labels = [l for l in all_annotations if l != 'hand']\n",
    "print(del_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把含有无用标签的`meta.json`修改，去掉无用标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_labels(meta_path, del_labels):\n",
    "    \"\"\"Remove the specified labels from meta.json\n",
    "    \"\"\"\n",
    "    with open(meta_path, 'r') as fp:\n",
    "        meta_content = json.load(fp)\n",
    "    \n",
    "    rewrite = False\n",
    "    classes = meta_content['classes']   # classes: list of dicts\n",
    "    new_classes = []\n",
    "    for cls_dict in classes:\n",
    "        if cls_dict['title'] in del_labels:\n",
    "            rewrite = True\n",
    "        else:\n",
    "            new_classes.append(cls_dict)\n",
    "    \n",
    "    if rewrite:\n",
    "        meta_content['classes'] = new_classes\n",
    "        print(\"Rewrote {} by deleting the labels: {}.\".format(meta_path, del_labels))\n",
    "        with open(meta_path, 'w') as fp:\n",
    "            json.dump(meta_content, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rewrote ../datasets/supervisely\\P01\\meta.json by deleting the labels: ['left_hand', 'right_hand', '1'].\n"
    }
   ],
   "source": [
    "for p in os.listdir(DS_PATH):\n",
    "    filepath = os.path.join(DS_PATH, p)\n",
    "    # print('\\nPerson {}: '.format(p), filepath)\n",
    "\n",
    "    if os.path.isdir(filepath):\n",
    "        # Read meta.json to get classes\n",
    "        meta_path = filepath + os.sep + 'meta.json'\n",
    "        if os.path.isfile(meta_path):\n",
    "            remove_labels(meta_path, del_labels)\n",
    "        else:\n",
    "            print('There is no meta.json file for person ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nSave labels for person/subset/images in ./ds_class_names.json.\nP01 :['hand']\nP02 :['hand']\nP03 :['hand']\nP04 :['hand']\nP05 :['hand']\nP06 :['hand']\nP07 :['hand']\nP08 :['hand']\nP10 :['hand']\nP12 :['hand']\nP13 :['hand']\nP14 :['hand']\nP15 :['hand']\nP16 :['hand']\nP17 :['hand']\nP19 :['hand']\nP20 :['hand']\nP21 :['hand']\nP22 :['hand']\nP23 :['hand']\n"
    }
   ],
   "source": [
    "# 检查一下数据集是不是清理干净了\n",
    "# meta.json\n",
    "class_names = {}    # Class names for person\n",
    "\n",
    "for p in os.listdir(DS_PATH):\n",
    "    filepath = os.path.join(DS_PATH, p)\n",
    "    # print('\\nPerson {}: '.format(p), filepath)\n",
    "\n",
    "    if os.path.isdir(filepath):\n",
    "        # Read meta.json to get classes\n",
    "        meta_path = filepath + os.sep + 'meta.json'\n",
    "        if os.path.isfile(meta_path):\n",
    "            classes, names_path = get_classes(meta_path, write=False)\n",
    "            class_names[p] = classes\n",
    "        else:\n",
    "            print('There is no meta.json file for person ', p)\n",
    "\n",
    "# Save class names of all datasets into json\n",
    "# class_names_path = DS_PATH + './ds_class_names.json'\n",
    "class_names_path = './ds_class_names.json'\n",
    "print('\\nSave labels for person/subset/images in {}.'.format(class_names_path))\n",
    "with open(class_names_path, 'w') as fp:\n",
    "    json.dump(class_names, fp, indent=4)\n",
    "\n",
    "# Iterate over key/value pairs in dict and print them\n",
    "for p, clss in class_names.items():\n",
    "    print(p, ' : ', clss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['hand']"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "# Get all labels from class_names (for all person)\n",
    "all_annotations = []\n",
    "for clss in class_names.values():\n",
    "    all_annotations.extend(clss)\n",
    "all_annotations = list(set(all_annotations))\n",
    "all_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Image exists with annotation: 'hand'\n"
    }
   ],
   "source": [
    "# Get all labels from class_names (for all person)\n",
    "all_annotations = []\n",
    "for clss in class_names.values():\n",
    "    all_annotations.extend(clss)\n",
    "all_annotations = set(all_annotations)\n",
    "for annotation in all_annotations:\n",
    "    if not find_annotation(labels, annotation):\n",
    "        print(\"No image with annotation: '{}'\".format(annotation))\n",
    "    else:\n",
    "        print(\"Image exists with annotation: '{}'\".format(annotation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在保证数据集是纯净的了，所有的图片有且仅有`hand`和没有`hand`两类，现在我们需要\n",
    "\n",
    "**划分训练集，验证集和测试集**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先把各个person的subsets混合\n",
    "\n",
    "在`../datasets`目录下创建`handset`目录，其中包含所有的person的目录`P01`-`P23`这样的。每一个person的目录下包含`images`和`labels`两个目录和一个文件`meta.json`。即\n",
    "```bash\n",
    "├───handset\n",
    "├───├───P01\n",
    "├───├───├───images\n",
    "├───├───├───labels\n",
    "├───├───├───meta.json\n",
    "├───├───P02\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERTED_PATH = '../datasets/handset'  # The path to the handset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Person: 100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n"
    }
   ],
   "source": [
    "# Make person(images and labels) folders\n",
    "with open('labels.json', 'r') as fp:\n",
    "    labels = json.load(fp)\n",
    "\n",
    "for p, p_set in tqdm(labels.items(), desc='Person'):\n",
    "    make_folders(\n",
    "        path=CONVERTED_PATH + os.sep + p    # like handset/person/images\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Person: 100%|██████████| 20/20 [1:08:51<00:00, 206.55s/it]\n"
    }
   ],
   "source": [
    "# Copy the images to there\n",
    "# But change the image name as subset_index.jpg such as P01_02_0000000001.jpg\n",
    "for p, p_set in tqdm(labels.items(), desc='Person'):\n",
    "    for sub_p, sub_p_set in p_set.items():\n",
    "        for img_name, bboxes in sub_p_set.items():\n",
    "            if len(bboxes) != 0:    # There is >=1 bboxes\n",
    "                # Reset file name\n",
    "                # image_path = CONVERTED_PATH + '/' + p + '/images/'\n",
    "                # label_path = CONVERTED_PATH + '/' + p + '/labels/'\n",
    "                image_path = os.path.join(CONVERTED_PATH, p, 'images/')\n",
    "                label_path = os.path.join(CONVERTED_PATH, p, 'labels/')\n",
    "                image_name = sub_p + '_' + img_name + '.jpg'\n",
    "                label_name = sub_p + '_' + img_name + '.txt'\n",
    "\n",
    "                # Move image to the folder\n",
    "                # curr_image_path = DS_PATH + os.sep + p + os.sep + sub_p + '/img/' + img_name + '.jpg'\n",
    "                curr_image_path = os.path.join(DS_PATH, p, sub_p, 'img', img_name + '.jpg')\n",
    "                shutil.copy(curr_image_path, image_path)\n",
    "                # Rename\n",
    "                os.rename(os.path.join(image_path, img_name+'.jpg'), os.path.join(image_path, image_name))\n",
    "                \n",
    "                for bbox in bboxes: # Each bbox: [class_name, b_x_center, b_y_center, b_width, b_height]\n",
    "                    # Get attributes\n",
    "                    class_index = all_annotations.index(bbox[0])\n",
    "                    b_x_center, b_y_center, b_width, b_height = bbox[1:]\n",
    "\n",
    "                    # Write labels file\n",
    "                    if (b_width > 0.) and (b_height > 0.):\n",
    "                        with open(label_path + label_name, 'a') as label_f:\n",
    "                            label_f.write('%d %.6f %.6f %.6f %.6f\\n' % (class_index, b_x_center, b_y_center, b_width, b_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test shutil.copy and os.rename\n",
    "with open('test.txt', 'w') as fp:\n",
    "    fp.write('Hello world!')\n",
    "\n",
    "dest_dir = os.mkdir('dest')\n",
    "dest_dir = 'dest'\n",
    "\n",
    "shutil.copy('test.txt', dest_dir)\n",
    "dest_file = os.path.join(dest_dir, 'test.txt')\n",
    "new_dst_file_name = os.path.join(dest_dir, 'yes.txt')\n",
    "os.rename(dest_file, new_dst_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'f:\\\\PROJECT\\\\project-diary\\\\classes.names'"
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "# Write classes.names \n",
    "with open(os.path.join(CONVERTED_PATH, 'classes.names'), 'w') as nf:\n",
    "    nf.write('{}\\n'.format('hand'))\n",
    "shutil.copy(os.path.join(CONVERTED_PATH, 'classes.names'), os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我已经有了所有的数据集，按person划分，以及写有唯一标签`hand`的文件`classes.names`。现在需要进行**数据集的划分**，有两种思路可以实践：\n",
    "\n",
    "1. 将所有person的数据集混合在一起，也就是把图片放在同一文件夹里面。然后随机打乱，去除一定比例的训练集，验证集和测试集；\n",
    "\n",
    "2. 不混合person的数据集，而是随机选择一定比例的person，将他们的数据集分别混合在一起，作为训练集，验证集和测试集。\n",
    "\n",
    "第一种方式操作起来很粗暴简单，但是会把所有人的图片混合在一起，随机取出之后各个subsets之间差别不大，在训练集上训练好的模型在测试集上可能一样表现很好，对于检测并提高模型的generalization没有帮助。因为每个人拍照的方式都会不同，数据集之间会存在一些特征差异，如果subsets各采用不同人群的数据会好一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(directory):\n",
    "    \"\"\"Get paths to all files (NOT folders) in a directory.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    for p in os.listdir(directory):\n",
    "        p_path = os.path.join(directory, p)\n",
    "        if os.path.isfile(p_path):\n",
    "            file_paths.append(p_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_paths(directory):\n",
    "    \"\"\"Get paths to all folders (NOT files) in a directory.\n",
    "    \"\"\"\n",
    "    folder_paths = []\n",
    "    for p in os.listdir(directory):\n",
    "        p_path = os.path.join(directory, p)\n",
    "        if os.path.isdir(p_path):\n",
    "            folder_paths.append(p_path)\n",
    "    return folder_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of paths to all person datasets\n",
    "p_paths = get_folder_paths(CONVERTED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['../datasets/handset\\\\P01',\n '../datasets/handset\\\\P02',\n '../datasets/handset\\\\P03',\n '../datasets/handset\\\\P04',\n '../datasets/handset\\\\P05',\n '../datasets/handset\\\\P06',\n '../datasets/handset\\\\P07',\n '../datasets/handset\\\\P08',\n '../datasets/handset\\\\P10',\n '../datasets/handset\\\\P12',\n '../datasets/handset\\\\P13',\n '../datasets/handset\\\\P14',\n '../datasets/handset\\\\P15',\n '../datasets/handset\\\\P16',\n '../datasets/handset\\\\P17',\n '../datasets/handset\\\\P19',\n '../datasets/handset\\\\P20',\n '../datasets/handset\\\\P21',\n '../datasets/handset\\\\P22',\n '../datasets/handset\\\\P23']"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "p_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 12345678\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of person for training set, validation set and test set\n",
    "train_ids, val_ids, test_ids = split_indices(\n",
    "    data=p_paths,\n",
    "    train_ratio=0.6,\n",
    "    val_ratio=0.2,\n",
    "    shuffle=True\n",
    ")\n",
    "datasets = {'train': train_ids, 'validation': val_ids, 'test': test_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'train': array([17,  5, 12,  3,  0, 13,  7,  9, 14, 10,  6, 15]),\n 'validation': array([18,  8,  1, 11]),\n 'test': array([16,  4,  2, 19])}"
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix datasets of individuals for each subset\n",
    "FINAL_PATH = '../datasets/ego-hand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'../datasets/ego-hand\\\\classes.names'"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "# Make images and labels folders\n",
    "make_folders(FINAL_PATH)\n",
    "shutil.copy(os.path.join(CONVERTED_PATH, 'classes.names'), FINAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在想了想，最后也还是得把所有照片混在一起，只要指明训练集/验证集/测试集还是按人分就行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer from handset to ego-hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the paths of subsets *.txt files\n",
    "subsets_paths = {}\n",
    "for key, ids in tqdm(datasets.items(), desc='Subsets'):\n",
    "    if ids.any():\n",
    "        subsets_paths[key] = os.path.join(CONVERTED_PATH, key + '.txt')  # Store the key/path\n",
    "        with open(subsets_paths[key], 'a') as wf:\n",
    "            for idx in ids: # idx: index of person\n",
    "                p_path = p_paths[idx]\n",
    "                # img_paths = get_file_paths(os.path.join(p_path, 'images'))\n",
    "                img_paths = sorted(glob.glob(p_path + 'images/' + '*.jpg'))\n",
    "                for img_path in img_paths:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Write .data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593863544646",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}